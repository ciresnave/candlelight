[package]
    authors = ["Lightbulb Contributors"]
    categories = ["mathematics", "science"]
    description = "Unified wrapper for the Candle ML framework ecosystem"
    documentation = "https://docs.rs/candlelight"
    edition = "2021"
    homepage = "https://github.com/ciresnave/candlelight"
    keywords = [
        "candle",
        "deep-learning",
        "machine-learning",
        "neural-networks",
        "pytorch",
    ]
    license = "MIT OR Apache-2.0"
    name = "candlelight"
    readme = "README.md"
    repository = "https://github.com/ciresnave/candlelight"
    version = "0.2.1"

[features]
    # "It just works" philosophy: all features enabled by default
    # Users can opt-out with `default-features = false` if needed
    default = [
        "flash-attn",
        "layer-norm",
        "datasets",
        "optimizers",
        "basin-hopping",
    ]

    # Hardware acceleration
    cuda = ["candle-core/cuda", "candle-nn/cuda", "candle-transformers/cuda"]
    metal = [
        "candle-core/metal",
        "candle-nn/metal",
        "candle-transformers/metal",
    ]

    # CUDA optimizations (require cuda feature)
    flash-attn = ["candle-flash-attn", "cuda"]
    layer-norm = ["candle-layer-norm", "cuda"]

    # Convenience: enable all CUDA optimizations
    cuda-full = ["cuda", "flash-attn", "layer-norm"]

    # Data loading utilities
    datasets = ["candle-datasets"]
    
    # Advanced optimizers
    optimizers = ["candle-optimisers"]
    
    # Global optimization algorithms
    basin-hopping = ["candle-bhop"]

[dependencies]
    # Core Candle framework - using git snapshot for CUDA 13.0 support via cudarc 0.17.1+
    # Temporary: Will switch to stable releases once Candle v0.10 is published
    candle-core         = { git = "https://github.com/huggingface/candle", rev = "db08cc0a5a786e00f873c35ced7db51fd7d7083a" }
    candle-nn           = { git = "https://github.com/huggingface/candle", rev = "db08cc0a5a786e00f873c35ced7db51fd7d7083a" }
    candle-transformers = { git = "https://github.com/huggingface/candle", rev = "db08cc0a5a786e00f873c35ced7db51fd7d7083a" }

    # Optional CUDA accelerations
    candle-flash-attn = { git = "https://github.com/huggingface/candle", rev = "db08cc0a5a786e00f873c35ced7db51fd7d7083a", optional = true }

    # Fused LayerNorm/RMSNorm - using our fork with updated dependencies for CUDA 13.0 compatibility + Windows large object fix
    # PR will be submitted to upstream: https://github.com/EricLBuehler/candle-layer-norm
    candle-layer-norm = { git = "https://github.com/ciresnave/candle-layer-norm", optional = true }

    # Data utilities (optional)
    candle-datasets = { git = "https://github.com/huggingface/candle", rev = "db08cc0a5a786e00f873c35ced7db51fd7d7083a", optional = true }
    
    # Additional optimizers - using our fork with updated dependencies for CUDA 13.0 compatibility
    # PR submitted to upstream: https://github.com/KGrewal1/optimisers/pull/29
    candle-optimisers = { git = "https://github.com/ciresnave/candle-optimisers", branch = "update-candle-deps-for-cuda-13", optional = true }
    
    # Basin hopping global optimization - using our fork with updated dependencies
    # PR submitted to upstream: https://github.com/KGrewal1/candle-bhop/pull/1
    candle-bhop = { git = "https://github.com/ciresnave/candle-bhop", branch = "update-to-candle-0.9", optional = true }
